{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f10c28",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d57851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:15.340836Z",
     "start_time": "2024-06-15T17:07:14.174864Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import sampler\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf9cf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:15.343940Z",
     "start_time": "2024-06-15T17:07:15.342154Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc2b297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:19.756300Z",
     "start_time": "2024-06-15T17:07:15.344724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liliyaivannikova/Documents/project/git/RecSys_Films/recsyc_part1\n",
      "\n",
      "movies: (86537, 3)\n",
      "rating: (33832162, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us = os.getcwd()\n",
    "print(us)\n",
    "\n",
    "if 'liliyaivannikova' in us:\n",
    "    PATH = r'/Users/liliyaivannikova/Documents/project/ml-latest/'\n",
    "    movies = pd.read_csv(PATH + r'movies.csv')\n",
    "    rating = pd.read_csv(PATH + r'ratings.csv')\n",
    "elif 'Владислав' in us:\n",
    "    movies = pd.read_csv(r'dataset/movies.csv')\n",
    "    rating = pd.read_csv(r'dataset/ratings.csv')\n",
    "    \n",
    "print(f'''\n",
    "movies: {movies.shape}\n",
    "rating: {rating.shape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9129d676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:20.499484Z",
     "start_time": "2024-06-15T17:07:19.757659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/9t70d84n0wbgzdftsf9jkvtw0000gn/T/ipykernel_6504/1871982909.py:15: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  movies['GENRES'] = movies['GENRES'].str.replace('|', ',')\n"
     ]
    }
   ],
   "source": [
    "movies.columns = [col.upper() for col in movies.columns]\n",
    "movies['REALEASE'] = movies['TITLE'].str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "movies['REALEASE'] = pd.to_datetime(movies['REALEASE'], format='%Y')\n",
    "movies['REALEASE'] = movies['REALEASE'].dt.year\n",
    "movies['TITLE'] = movies['TITLE'].str[:-7]\n",
    "\n",
    "# dummy-кодирование жанров\n",
    "dfx = movies['GENRES'].str.get_dummies(sep='|')\n",
    "for col in dfx.columns:\n",
    "    dfx[col] = dfx[col].astype('int8')\n",
    "    \n",
    "movies = pd.concat([movies, dfx], axis=1) #.drop(columns=['GENRES'])\n",
    "movies['TITLE'] = movies['TITLE'].astype('category')\n",
    "movies['REALEASE'] = movies['REALEASE'].astype('float16')\n",
    "movies['GENRES'] = movies['GENRES'].str.replace('|', ',')\n",
    "\n",
    "rating.columns = [col.upper() for col in rating.columns]\n",
    "rating['TIMESTAMP'] = pd.to_datetime(rating['TIMESTAMP'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e78a9",
   "metadata": {},
   "source": [
    "# Фильтрация строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14787f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:20.765857Z",
     "start_time": "2024-06-15T17:07:20.500324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считаем кол-во оценок в рамках userid\n",
    "rating_stat = rating.groupby('USERID')['MOVIEID'].count().reset_index()\n",
    "rating_stat['MOVIEID'].quantile(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5387f24d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:22.388398Z",
     "start_time": "2024-06-15T17:07:20.766728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERID</th>\n",
       "      <th>MOVIEID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>FLAG_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008-11-03 17:52:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008-11-05 06:04:46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008-11-03 17:31:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2008-11-03 18:00:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2008-11-03 17:58:39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USERID  MOVIEID  RATING           TIMESTAMP  FLAG_05\n",
       "0       1        1     4.0 2008-11-03 17:52:19        1\n",
       "1       1      110     4.0 2008-11-05 06:04:46        1\n",
       "2       1      158     4.0 2008-11-03 17:31:43        1\n",
       "3       1      260     4.5 2008-11-03 18:00:04        1\n",
       "4       1      356     5.0 2008-11-03 17:58:39        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rating_stat['FLAG_05'] = np.where(rating_stat['MOVIEID'] > rating_stat['MOVIEID'].quantile(0.05), 1, 0)\n",
    "'''при текущих симуляцих мы отфильтровали наш набор данных, оставив для построения только userid, которые\n",
    "оценили больше 20 фильмов\n",
    "'''\n",
    "rating_stat['FLAG_05'] = np.where(rating_stat['MOVIEID'] >= rating_stat['MOVIEID'].quantile(0.4), 1, 0)\n",
    "rating = rating.merge(rating_stat[['USERID', 'FLAG_05']], how = 'left', on = 'USERID')\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47eb771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:24.397618Z",
     "start_time": "2024-06-15T17:07:22.389247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32346391, 5)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "По результатам EDA\n",
    "'''\n",
    "# удаляем пропуски\n",
    "rating = rating[~((rating.TIMESTAMP.isna())|(rating.USERID.isna())|(rating.RATING.isna()))]\n",
    "# удаляем странного юзера \n",
    "rating.query('USERID != 189614', inplace = True)\n",
    "# удалим юзеров, у которых оценок меньше 3 (по 5 квантилю)\n",
    "rating.query('FLAG_05 == 1', inplace = True)\n",
    "print(rating.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddc7ad",
   "metadata": {},
   "source": [
    "# train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358a1b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:07:24.401100Z",
     "start_time": "2024-06-15T17:07:24.398428Z"
    }
   },
   "outputs": [],
   "source": [
    "rating.rename({'USERID':'user_id',\n",
    "            'MOVIEID':'item_id',\n",
    "            'RATING':'weight',\n",
    "            'TIMESTAMP':'datetime'}, axis=1, inplace=True)\n",
    "\n",
    "movies.rename({'MOVIEID':'item_id'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ca74a3be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T07:27:44.347436Z",
     "start_time": "2024-06-16T07:27:44.312074Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CriteoDataset(Dataset):\n",
    "     \n",
    "    def __init__(self, data, train=True):\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "        if not self._check_exists:\n",
    "            raise RuntimeError('Dataset not found.')\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data = data.iloc[:, :-1].values\n",
    "            self.target = data.iloc[:, -1].values\n",
    "        else:\n",
    "            self.test_data = data.iloc[:, :-1].values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            dataI, targetI = self.train_data[idx, :], self.target[idx]\n",
    "            Xi = torch.from_numpy(dataI.astype(np.int32)).unsqueeze(-1)\n",
    "            Xv = torch.from_numpy(np.ones_like(dataI))\n",
    "            return Xi, Xv, targetI\n",
    "        else:\n",
    "            dataI = self.test_data.iloc[idx, :]\n",
    "            Xi = torch.from_numpy(dataI.astype(np.int32)).unsqueeze(-1)\n",
    "            Xv = torch.from_numpy(np.ones_like(dataI))\n",
    "            return Xi, Xv\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(self.root)\n",
    "    \n",
    "    \n",
    "class DeepFM(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_sizes, embedding_size=4, k=3,\n",
    "                 hidden_dims=[32, 32], num_classes=1, dropout=[0.5, 0.5], \n",
    "                 use_cuda=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize a new network\n",
    "\n",
    "        Inputs: \n",
    "        - feature_size: A list of integer giving the size of features for each field.\n",
    "        - embedding_size: An integer giving size of feature embedding.\n",
    "        - hidden_dims: A list of integer giving the size of each hidden layer.\n",
    "        - num_classes: An integer giving the number of classes to predict. For example,\n",
    "                    someone may rate 1,2,3,4 or 5 stars to a film.\n",
    "        - batch_size: An integer giving size of instances used in each interation.\n",
    "        - use_cuda: Bool, Using cuda or not\n",
    "        - verbose: Bool\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.field_size = len(feature_sizes)\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_classes = num_classes\n",
    "        self.dtype = torch.long\n",
    "        self.bias = torch.nn.Parameter(torch.randn(1))\n",
    "        self.k = k\n",
    "        self.num_correct = 0\n",
    "        self.num_samples = 0\n",
    "        \"\"\"\n",
    "            check if use cuda\n",
    "        \"\"\"\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \"\"\"\n",
    "            init fm part\n",
    "        \"\"\"\n",
    "        self.fm_first_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, 1) for feature_size in self.feature_sizes])\n",
    "        self.fm_second_order_embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(feature_size, self.embedding_size) for feature_size in self.feature_sizes])\n",
    "        \"\"\"\n",
    "            init deep part\n",
    "        \"\"\"\n",
    "        all_dims = [self.field_size * self.embedding_size] + \\\n",
    "            self.hidden_dims + [self.num_classes]\n",
    "        for i in range(1, len(hidden_dims) + 1):\n",
    "            setattr(self, 'linear_'+str(i),\n",
    "                    nn.Linear(all_dims[i-1], all_dims[i]))\n",
    "            # nn.init.kaiming_normal_(self.fc1.weight)\n",
    "            setattr(self, 'batchNorm_' + str(i),\n",
    "                    nn.BatchNorm1d(all_dims[i]))\n",
    "            setattr(self, 'dropout_'+str(i),\n",
    "                    nn.Dropout(dropout[i-1]))\n",
    "\n",
    "    def forward(self, Xi, Xv):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - Xi: A tensor of input's index, shape of (N, field_size, 1)\n",
    "        - Xv: A tensor of input's value, shape of (N, field_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        fm_first_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_first_order_embeddings)]\n",
    "        fm_first_order = torch.cat(fm_first_order_emb_arr, 1)\n",
    "        fm_second_order_emb_arr = [(torch.sum(emb(Xi[:, i, :]), 1).t() * Xv[:, i]).t() for i, emb in enumerate(self.fm_second_order_embeddings)]\n",
    "        fm_sum_second_order_emb = sum(fm_second_order_emb_arr)\n",
    "        fm_sum_second_order_emb_square = fm_sum_second_order_emb * \\\n",
    "            fm_sum_second_order_emb  # (x+y)^2\n",
    "        fm_second_order_emb_square = [\n",
    "            item*item for item in fm_second_order_emb_arr]\n",
    "        fm_second_order_emb_square_sum = sum(\n",
    "            fm_second_order_emb_square)  # x^2+y^2\n",
    "        fm_second_order = (fm_sum_second_order_emb_square -\n",
    "                           fm_second_order_emb_square_sum) * 0.5\n",
    "        \"\"\"\n",
    "            deep part\n",
    "        \"\"\"\n",
    "        deep_emb = torch.cat(fm_second_order_emb_arr, 1)\n",
    "        deep_out = deep_emb\n",
    "        for i in range(1, len(self.hidden_dims) + 1):\n",
    "            deep_out = getattr(self, 'linear_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'batchNorm_' + str(i))(deep_out)\n",
    "            deep_out = getattr(self, 'dropout_' + str(i))(deep_out)\n",
    "        \"\"\"\n",
    "            sum\n",
    "        \"\"\"\n",
    "        total_sum = torch.sum(fm_first_order, 1) + \\\n",
    "                    torch.sum(fm_second_order, 1) + torch.sum(deep_out, 1) + self.bias\n",
    "        return total_sum\n",
    "\n",
    "    def fit(self, loader_train, loader_val, optimizer, epochs=100, verbose=False, print_every=100):\n",
    "\n",
    "        model = self.train().to(device=self.device)\n",
    "        criterion = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for t, (xi, xv, y) in enumerate(loader_train):\n",
    "                xi = xi.to(device=self.device, dtype=self.dtype)\n",
    "                xv = xv.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.float)\n",
    "                \n",
    "                total = model(xi, xv)\n",
    "                loss = criterion(total, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if verbose and t % print_every == 0:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                    self.check_accuracy(loader_val, model)\n",
    "                    print()\n",
    "    \n",
    "    def check_accuracy(self, loader, model):\n",
    "        if loader.dataset.train:\n",
    "            print('Checking MAP@k on validation set')\n",
    "        else:\n",
    "            print('Checking MAP@k on test set')   \n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        model.eval()  # set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for xi, xv, y in loader:\n",
    "                xi = xi.to(device=self.device, dtype=self.dtype)  # move to device, e.g. GPU\n",
    "                xv = xv.to(device=self.device, dtype=torch.float)\n",
    "                y = y.to(device=self.device, dtype=torch.bool)\n",
    "                total = model(xi, xv)\n",
    "                preds_vals = F.sigmoid(total) \n",
    "                preds_idx = torch.topk(preds_vals.flatten(), self.k).indices\n",
    "                preds = (preds_vals[preds_idx] > 0.5)\n",
    "                y_small = y[preds_idx]\n",
    "                self.num_correct += (y_small).sum()\n",
    "                self.num_samples += preds.size(0)\n",
    "            mean_ap = float(self.num_correct) / self.num_samples\n",
    "            print('Got %d / %d correct (%.5f%%)' % (self.num_correct, self.num_samples, 100 * mean_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ef0cdb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:32:26.535214Z",
     "start_time": "2024-06-15T17:32:26.530941Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "13f2e11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T18:10:46.727615Z",
     "start_time": "2024-06-15T18:09:51.762047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30357121, 5), (1989270, 5))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "бьем на train/test по времени (20 последних оценок идут в test)\n",
    "'''\n",
    "rating.sort_values(['user_id', 'datetime'], inplace=True)\n",
    "\n",
    "train_ratings, test_ratings = [], []\n",
    "num_test_samples = 10\n",
    "\n",
    "for userId, user_data in rating.groupby('user_id'):\n",
    "    train_ratings += [user_data[:-num_test_samples]]\n",
    "    test_ratings += [user_data[-num_test_samples:]]\n",
    "\n",
    "train_ratings = pd.concat(train_ratings)\n",
    "test_ratings = pd.concat(test_ratings)\n",
    "train_ratings.shape, test_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "feec46eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T18:15:24.649789Z",
     "start_time": "2024-06-15T18:15:24.450015Z"
    }
   },
   "outputs": [],
   "source": [
    "test_target = test_ratings['weight'].copy()\n",
    "test_ratings.drop(['datetime', 'FLAG_05'], axis=1, inplace=True)\n",
    "train_target = train_ratings['weight'].copy()\n",
    "train_ratings.drop(['datetime', 'FLAG_05'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c44806c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T18:44:36.788160Z",
     "start_time": "2024-06-15T18:44:36.598307Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ratings['weight'] = np.where(train_ratings['weight']>3, 1, 0)\n",
    "test_ratings['weight'] = np.where(test_ratings['weight']>3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0f2b5dd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T18:12:00.275129Z",
     "start_time": "2024-06-15T18:12:00.263479Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6c3637da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T18:44:48.990744Z",
     "start_time": "2024-06-15T18:44:48.759614Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CriteoDataset(train_ratings, train=True)\n",
    "loader_train = DataLoader(train_data, batch_size=1028*8)\n",
    "val_data = CriteoDataset(test_ratings, train=True)\n",
    "loader_val = DataLoader(val_data, batch_size=1028*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa1ff1",
   "metadata": {},
   "source": [
    "# deepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1dfb5a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T19:36:22.506297Z",
     "start_time": "2024-06-15T18:44:52.300249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 6.0054\n",
      "Checking accuracy on validation set\n",
      "Got 939127 / 1989270 correct (47.21%)\n",
      "\n",
      "Iteration 100, loss = 3.7563\n",
      "Checking accuracy on validation set\n",
      "Got 939856 / 1989270 correct (47.25%)\n",
      "\n",
      "Iteration 200, loss = 3.8436\n",
      "Checking accuracy on validation set\n",
      "Got 940758 / 1989270 correct (47.29%)\n",
      "\n",
      "Iteration 300, loss = 3.5190\n",
      "Checking accuracy on validation set\n",
      "Got 941539 / 1989270 correct (47.33%)\n",
      "\n",
      "Iteration 400, loss = 3.6596\n",
      "Checking accuracy on validation set\n",
      "Got 942372 / 1989270 correct (47.37%)\n",
      "\n",
      "Iteration 500, loss = 3.7291\n",
      "Checking accuracy on validation set\n",
      "Got 943310 / 1989270 correct (47.42%)\n",
      "\n",
      "Iteration 600, loss = 3.6315\n",
      "Checking accuracy on validation set\n",
      "Got 944054 / 1989270 correct (47.46%)\n",
      "\n",
      "Iteration 700, loss = 3.8959\n",
      "Checking accuracy on validation set\n",
      "Got 944873 / 1989270 correct (47.50%)\n",
      "\n",
      "Iteration 800, loss = 3.4134\n",
      "Checking accuracy on validation set\n",
      "Got 945769 / 1989270 correct (47.54%)\n",
      "\n",
      "Iteration 900, loss = 3.5230\n",
      "Checking accuracy on validation set\n",
      "Got 946608 / 1989270 correct (47.59%)\n",
      "\n",
      "Iteration 1000, loss = 3.5204\n",
      "Checking accuracy on validation set\n",
      "Got 947525 / 1989270 correct (47.63%)\n",
      "\n",
      "Iteration 1100, loss = 3.6019\n",
      "Checking accuracy on validation set\n",
      "Got 948316 / 1989270 correct (47.67%)\n",
      "\n",
      "Iteration 1200, loss = 3.8302\n",
      "Checking accuracy on validation set\n",
      "Got 949143 / 1989270 correct (47.71%)\n",
      "\n",
      "Iteration 1300, loss = 3.6027\n",
      "Checking accuracy on validation set\n",
      "Got 949903 / 1989270 correct (47.75%)\n",
      "\n",
      "Iteration 1400, loss = 3.9032\n",
      "Checking accuracy on validation set\n",
      "Got 950756 / 1989270 correct (47.79%)\n",
      "\n",
      "Iteration 1500, loss = 3.3609\n",
      "Checking accuracy on validation set\n",
      "Got 951575 / 1989270 correct (47.84%)\n",
      "\n",
      "Iteration 1600, loss = 3.5182\n",
      "Checking accuracy on validation set\n",
      "Got 952425 / 1989270 correct (47.88%)\n",
      "\n",
      "Iteration 1700, loss = 3.7587\n",
      "Checking accuracy on validation set\n",
      "Got 953256 / 1989270 correct (47.92%)\n",
      "\n",
      "Iteration 1800, loss = 3.2473\n",
      "Checking accuracy on validation set\n",
      "Got 954086 / 1989270 correct (47.96%)\n",
      "\n",
      "Iteration 1900, loss = 3.5335\n",
      "Checking accuracy on validation set\n",
      "Got 954882 / 1989270 correct (48.00%)\n",
      "\n",
      "Iteration 2000, loss = 3.5372\n",
      "Checking accuracy on validation set\n",
      "Got 955650 / 1989270 correct (48.04%)\n",
      "\n",
      "Iteration 2100, loss = 3.5094\n",
      "Checking accuracy on validation set\n",
      "Got 956464 / 1989270 correct (48.08%)\n",
      "\n",
      "Iteration 2200, loss = 3.8080\n",
      "Checking accuracy on validation set\n",
      "Got 957285 / 1989270 correct (48.12%)\n",
      "\n",
      "Iteration 2300, loss = 3.6476\n",
      "Checking accuracy on validation set\n",
      "Got 958054 / 1989270 correct (48.16%)\n",
      "\n",
      "Iteration 2400, loss = 3.9938\n",
      "Checking accuracy on validation set\n",
      "Got 958741 / 1989270 correct (48.20%)\n",
      "\n",
      "Iteration 2500, loss = 3.4946\n",
      "Checking accuracy on validation set\n",
      "Got 959573 / 1989270 correct (48.24%)\n",
      "\n",
      "Iteration 2600, loss = 3.5844\n",
      "Checking accuracy on validation set\n",
      "Got 960326 / 1989270 correct (48.28%)\n",
      "\n",
      "Iteration 2700, loss = 3.4934\n",
      "Checking accuracy on validation set\n",
      "Got 961126 / 1989270 correct (48.32%)\n",
      "\n",
      "Iteration 2800, loss = 3.9786\n",
      "Checking accuracy on validation set\n",
      "Got 961814 / 1989270 correct (48.35%)\n",
      "\n",
      "Iteration 2900, loss = 3.7194\n",
      "Checking accuracy on validation set\n",
      "Got 962712 / 1989270 correct (48.40%)\n",
      "\n",
      "Iteration 3000, loss = 3.3376\n",
      "Checking accuracy on validation set\n",
      "Got 963506 / 1989270 correct (48.44%)\n",
      "\n",
      "Iteration 3100, loss = 3.5932\n",
      "Checking accuracy on validation set\n",
      "Got 964209 / 1989270 correct (48.47%)\n",
      "\n",
      "Iteration 3200, loss = 3.5909\n",
      "Checking accuracy on validation set\n",
      "Got 965048 / 1989270 correct (48.51%)\n",
      "\n",
      "Iteration 3300, loss = 3.6274\n",
      "Checking accuracy on validation set\n",
      "Got 965833 / 1989270 correct (48.55%)\n",
      "\n",
      "Iteration 3400, loss = 3.6154\n",
      "Checking accuracy on validation set\n",
      "Got 966686 / 1989270 correct (48.60%)\n",
      "\n",
      "Iteration 3500, loss = 3.5275\n",
      "Checking accuracy on validation set\n",
      "Got 967548 / 1989270 correct (48.64%)\n",
      "\n",
      "Iteration 3600, loss = 3.4773\n",
      "Checking accuracy on validation set\n",
      "Got 968278 / 1989270 correct (48.68%)\n",
      "\n",
      "Iteration 0, loss = 3.5898\n",
      "Checking accuracy on validation set\n",
      "Got 968910 / 1989270 correct (48.71%)\n",
      "\n",
      "Iteration 100, loss = 3.5597\n",
      "Checking accuracy on validation set\n",
      "Got 969671 / 1989270 correct (48.75%)\n",
      "\n",
      "Iteration 200, loss = 3.6144\n",
      "Checking accuracy on validation set\n",
      "Got 970549 / 1989270 correct (48.79%)\n",
      "\n",
      "Iteration 300, loss = 3.3237\n",
      "Checking accuracy on validation set\n",
      "Got 971295 / 1989270 correct (48.83%)\n",
      "\n",
      "Iteration 400, loss = 3.4098\n",
      "Checking accuracy on validation set\n",
      "Got 972166 / 1989270 correct (48.87%)\n",
      "\n",
      "Iteration 500, loss = 3.5715\n",
      "Checking accuracy on validation set\n",
      "Got 973074 / 1989270 correct (48.92%)\n",
      "\n",
      "Iteration 600, loss = 3.4767\n",
      "Checking accuracy on validation set\n",
      "Got 973774 / 1989270 correct (48.95%)\n",
      "\n",
      "Iteration 700, loss = 3.6482\n",
      "Checking accuracy on validation set\n",
      "Got 974638 / 1989270 correct (48.99%)\n",
      "\n",
      "Iteration 800, loss = 3.2912\n",
      "Checking accuracy on validation set\n",
      "Got 975452 / 1989270 correct (49.04%)\n",
      "\n",
      "Iteration 900, loss = 3.3564\n",
      "Checking accuracy on validation set\n",
      "Got 976121 / 1989270 correct (49.07%)\n",
      "\n",
      "Iteration 1000, loss = 3.3369\n",
      "Checking accuracy on validation set\n",
      "Got 976871 / 1989270 correct (49.11%)\n",
      "\n",
      "Iteration 1100, loss = 3.3827\n",
      "Checking accuracy on validation set\n",
      "Got 977627 / 1989270 correct (49.15%)\n",
      "\n",
      "Iteration 1200, loss = 3.5841\n",
      "Checking accuracy on validation set\n",
      "Got 978414 / 1989270 correct (49.18%)\n",
      "\n",
      "Iteration 1300, loss = 3.3369\n",
      "Checking accuracy on validation set\n",
      "Got 979154 / 1989270 correct (49.22%)\n",
      "\n",
      "Iteration 1400, loss = 3.6463\n",
      "Checking accuracy on validation set\n",
      "Got 979973 / 1989270 correct (49.26%)\n",
      "\n",
      "Iteration 1500, loss = 3.2379\n",
      "Checking accuracy on validation set\n",
      "Got 980839 / 1989270 correct (49.31%)\n",
      "\n",
      "Iteration 1600, loss = 3.4594\n",
      "Checking accuracy on validation set\n",
      "Got 981536 / 1989270 correct (49.34%)\n",
      "\n",
      "Iteration 1700, loss = 3.5411\n",
      "Checking accuracy on validation set\n",
      "Got 982335 / 1989270 correct (49.38%)\n",
      "\n",
      "Iteration 1800, loss = 3.1243\n",
      "Checking accuracy on validation set\n",
      "Got 983130 / 1989270 correct (49.42%)\n",
      "\n",
      "Iteration 1900, loss = 3.3726\n",
      "Checking accuracy on validation set\n",
      "Got 983785 / 1989270 correct (49.45%)\n",
      "\n",
      "Iteration 2000, loss = 3.4072\n",
      "Checking accuracy on validation set\n",
      "Got 984410 / 1989270 correct (49.49%)\n",
      "\n",
      "Iteration 2100, loss = 3.3386\n",
      "Checking accuracy on validation set\n",
      "Got 985098 / 1989270 correct (49.52%)\n",
      "\n",
      "Iteration 2200, loss = 3.5310\n",
      "Checking accuracy on validation set\n",
      "Got 985896 / 1989270 correct (49.56%)\n",
      "\n",
      "Iteration 2300, loss = 3.4518\n",
      "Checking accuracy on validation set\n",
      "Got 986690 / 1989270 correct (49.60%)\n",
      "\n",
      "Iteration 2400, loss = 3.6755\n",
      "Checking accuracy on validation set\n",
      "Got 987293 / 1989270 correct (49.63%)\n",
      "\n",
      "Iteration 2500, loss = 3.3122\n",
      "Checking accuracy on validation set\n",
      "Got 987931 / 1989270 correct (49.66%)\n",
      "\n",
      "Iteration 2600, loss = 3.4268\n",
      "Checking accuracy on validation set\n",
      "Got 988586 / 1989270 correct (49.70%)\n",
      "\n",
      "Iteration 2700, loss = 3.3644\n",
      "Checking accuracy on validation set\n",
      "Got 989312 / 1989270 correct (49.73%)\n",
      "\n",
      "Iteration 2800, loss = 3.6916\n",
      "Checking accuracy on validation set\n",
      "Got 989961 / 1989270 correct (49.77%)\n",
      "\n",
      "Iteration 2900, loss = 3.4404\n",
      "Checking accuracy on validation set\n",
      "Got 990790 / 1989270 correct (49.81%)\n",
      "\n",
      "Iteration 3000, loss = 3.2164\n",
      "Checking accuracy on validation set\n",
      "Got 991605 / 1989270 correct (49.85%)\n",
      "\n",
      "Iteration 3100, loss = 3.3858\n",
      "Checking accuracy on validation set\n",
      "Got 992337 / 1989270 correct (49.88%)\n",
      "\n",
      "Iteration 3200, loss = 3.4693\n",
      "Checking accuracy on validation set\n",
      "Got 993083 / 1989270 correct (49.92%)\n",
      "\n",
      "Iteration 3300, loss = 3.4637\n",
      "Checking accuracy on validation set\n",
      "Got 993785 / 1989270 correct (49.96%)\n",
      "\n",
      "Iteration 3400, loss = 3.5021\n",
      "Checking accuracy on validation set\n",
      "Got 994640 / 1989270 correct (50.00%)\n",
      "\n",
      "Iteration 3500, loss = 3.3697\n",
      "Checking accuracy on validation set\n",
      "Got 995421 / 1989270 correct (50.04%)\n",
      "\n",
      "Iteration 3600, loss = 3.3325\n",
      "Checking accuracy on validation set\n",
      "Got 996232 / 1989270 correct (50.08%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "feature_sizes = [500000, 500000]\n",
    "\n",
    "model = DeepFM(feature_sizes, embedding_size=64,\n",
    "                 hidden_dims=[64, 64], num_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=0.0)\n",
    "model.fit(loader_train, loader_val, optimizer, epochs=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ece7d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T07:27:46.642Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.87 µs\n",
      "Iteration 0, loss = 5.5296\n",
      "Checking MAP@k on validation set\n",
      "Got 1640 / 2420 correct (67.76860%)\n",
      "\n",
      "Iteration 100, loss = 3.3187\n",
      "Checking MAP@k on validation set\n",
      "Got 3284 / 4840 correct (67.85124%)\n",
      "\n",
      "Iteration 200, loss = 3.2984\n",
      "Checking MAP@k on validation set\n",
      "Got 4934 / 7260 correct (67.96143%)\n",
      "\n",
      "Iteration 300, loss = 3.4739\n",
      "Checking MAP@k on validation set\n",
      "Got 6583 / 9680 correct (68.00620%)\n",
      "\n",
      "Iteration 400, loss = 3.5595\n",
      "Checking MAP@k on validation set\n",
      "Got 8230 / 12100 correct (68.01653%)\n",
      "\n",
      "Iteration 500, loss = 4.1160\n",
      "Checking MAP@k on validation set\n",
      "Got 9876 / 14520 correct (68.01653%)\n",
      "\n",
      "Iteration 600, loss = 3.4665\n",
      "Checking MAP@k on validation set\n",
      "Got 11521 / 16940 correct (68.01063%)\n",
      "\n",
      "Iteration 700, loss = 3.2038\n",
      "Checking MAP@k on validation set\n",
      "Got 13168 / 19360 correct (68.01653%)\n",
      "\n",
      "Iteration 800, loss = 3.3791\n",
      "Checking MAP@k on validation set\n",
      "Got 14817 / 21780 correct (68.03030%)\n",
      "\n",
      "Iteration 900, loss = 3.2501\n",
      "Checking MAP@k on validation set\n",
      "Got 16465 / 24200 correct (68.03719%)\n",
      "\n",
      "Iteration 1000, loss = 3.2010\n",
      "Checking MAP@k on validation set\n",
      "Got 18118 / 26620 correct (68.06161%)\n",
      "\n",
      "Iteration 1100, loss = 3.4444\n",
      "Checking MAP@k on validation set\n",
      "Got 19769 / 29040 correct (68.07507%)\n",
      "\n",
      "Iteration 1200, loss = 3.2983\n",
      "Checking MAP@k on validation set\n",
      "Got 21419 / 31460 correct (68.08328%)\n",
      "\n",
      "Iteration 1300, loss = 3.0504\n",
      "Checking MAP@k on validation set\n",
      "Got 23069 / 33880 correct (68.09032%)\n",
      "\n",
      "Iteration 1400, loss = 3.3290\n",
      "Checking MAP@k on validation set\n",
      "Got 24720 / 36300 correct (68.09917%)\n",
      "\n",
      "Iteration 1500, loss = 3.4365\n",
      "Checking MAP@k on validation set\n",
      "Got 26372 / 38720 correct (68.10950%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "feature_sizes = [500000, 500000]\n",
    "\n",
    "model = DeepFM(feature_sizes, embedding_size=64, k=10,\n",
    "                 hidden_dims=[64, 64], num_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=0.0)\n",
    "model.fit(loader_train, loader_val, optimizer, epochs=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c037cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf9295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
